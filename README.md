# Info Track - Demo Take Home Assignment

## Welcome

This repository contains my F# ↔ C# implementation for the take-home assignment. I chose F# for the primary implementation because the requirement specified a .NET 8 application, and the architecture is designed to interoperate cleanly with C# through explicit interfaces and shared contracts.

The codebase is structured so that core services and behaviors are exposed via interfaces, making the same logic easily consumable from C# if this were extended or migrated in the future.

Additional Packages:

- Added DotNetEnv for configuration via a `.env` file during local development, while still supporting standard environment variables for production deployments. This makes API key management explicit and easy to reproduce when running the project locally.

The project uses Fable Remoting for client–server communication. This allows both the client and server to share the same request and response types (see `Shared/Shared.fs`), ensuring compile-time safety and eliminating manual DTO duplication.

On the server side, dependency injection is used to register the discovery service, query extractor(s), and HTTP clients. Named `HttpClient`s are configured to keep external concerns (LLM providers, Open Library) isolated and configurable, following recommended .NET practices.

---

# General Workflow Explanation:

1.) User enters some text to input field and clicks submit or enter to submit as a query to discover API.
2.) API receives and then:
    i.) Uses an LLM to extract structured hypotheses (possible title, author, keywords, year) from the raw user query
    ii.)    Build openLibrary query string
    iii.)   Search OpenLibrary
    iv.)    Resolve the works details and primary authors using openLibrary endpoints
    v.) Rank and explain results using deterministic signals (title match, author role, publication year). Explanations are generated by the system rather than the LLM to avoid hallucinated reasoning.
    vi.)    Map to the response type
    vii.)   Send response back to client
3.) Client receives and displays the ranked results to the user.

---
---

# Project Structure:

src
    |-Client - Client application
    |    * Files of Note:
    |        - Index.fs: Has the main view that is shown to the end user. This is using Fable and Feliz for the way we create components.
    |        - App.fs: configures and runs the application and loads the side effect for the styles.
    |
    |-Shared - Houses types that can be referenced and safely shared between the client and the server.
    |    * Files of Note:
    |        - Shared.fs: Has the types that are used as requests for Fable remoting front and backend communication (Client <-> Server)
    | 
    |-Server - Handles requests from client application, responsible for communicating with OpenAI API, Google Gemini API and OpenLibrary, processing and ranking the results with a grounded perspective based in logic, so cannot be hallucinated.
    |    AI/
    |       ChatGPT/
    |        - ChatGPTExtractor.fs: Houses type for communicating with OpenAI API, parsing the results and handling responses from their API based on status codes, with retry attempts and back off. 
    |       Gemini/
    |        - GeminiExtractor.fs: Houses type for communicating with Google Gemini API, parsing the results and handling responses from their API based on status codes, with retry attempts and back off. 
    |   - AiConfig.fs: Gathers the required information from .env variables to generate the options for interacting with the AI API.
    |   - ExtrationParsing.fs: Centralize some logic for parsing the Json API response.
    |   - ExtractorFactory.fs: Tries to parse results, and calls fallback extractor if primary parser fails for it's AI Json response.
    |   - FallbackExtractor.fs: Default fallback logic.
    |    Application/
    |       - Dicovery.fs: Heart of the logic and pipeline for handling the request and obligations for the requirements in one place.
    |    Core/
    |       - OpenLibraryCanonicalize.fs: Handling creatig records from OpenLibrary responses into a cohesive shape. 
    |       - OpenLibraryClient.fs: In charge of communication with OpenLibrary API
    |    Domain/
    |       - Domain.fs: Types used by the server internally and communicating with OpenLibrary.
    |    Endpoints/
    |       Discovery/
    |           - LibraryDiscovery.fs: Api for creating api with the webApp of the server (handles routing for us) see Server.fs.
    |           - LibraryDiscoveryService.fs: Interface contract used for the implementation on the server and client.
    |           - LibraryDiscoveryServiceImpl.fs: Server implementation calling the Application/Discovery.fs function to run the pipeline.
    |    Utils/
    |       - Http.fs: Helper function to be reused foo OpenAI and Google API retries with backoff, improvement would be use for openlibrary as well.
    |       - JsonExtractor.fs: Centralized helper functions for handling Json.
    |       - Normalize.fs: Nommralization helper functions when working with raw text.
    |    - Server.fs.fs: Configuration of the backend application. Uses Dependency Injection for the Http clients, adds singletons for factories to support the extraction and the service for handling discovery.


---

## .env configuration

```bash

# Choose which provider; currently accepts: chatgpt, openai (these are the same) or gemini
AI_PROVIDER=chatgpt
# AI_PROVIDER=gemini

# Configuration for Google Gemini API access. You must supply the API key if configuration is 'gemini'
GEMINI_API_KEY=AI...
# Configuration for Google Gemini API models, supports comma separated values for trying a few models, as I got rate limited heavily while writing this and there is a chance that the endpoint is overwhelmed.
GEMINI_MODELS=gemini-flash-latest,gemini-3-flash-preview,gemini-2.0-flash
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1

# Configuration for Google Gemini API access. You must supply the API key if configuration is 'chatgpt' || 'openai'
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1

```

# Running tests:

Shared & Server Tests: 

```bash

dotnet tool restore
dotnet paket restore

dotnet run test
```

Client Tests: 

```bash
cd tests/Client

# Run with script in package.json
npm run test:client
```

---

## Design Decisions & Tradeoffs

### Functional Core with Explicit Boundaries
The core discovery logic is implemented in a functional style with immutable data and explicit inputs/outputs. Side effects (HTTP calls, LLM usage) are isolated behind interfaces. This makes the ranking and explanation logic deterministic, testable, and easy to reason about independently of external services.

### AI as an Input Normalizer, Not a Decision Maker
LLMs are used strictly for hypothesis extraction (possible title, author, keywords, year) from unstructured input. All ranking, matching, and explanations are derived from concrete Open Library data rather than LLM-generated reasoning. This avoids hallucinated explanations and keeps final results grounded in verifiable fields.

### Canonicalization over Raw Search Results
Open Library search results often include multiple editions and contributors. The pipeline resolves results to canonical works and primary authors before ranking. Contributors (illustrators, editors, adaptors) are treated as lower-signal matches and explicitly called out in explanations when relevant.

### Deterministic Ranking with Explainability
Rather than using a single numeric relevance score, candidates are ranked using a hierarchy of matching signals (exact title, near title, primary author, contributor, year). This allows the system to generate concise, human-readable explanations that directly reflect why an item ranked higher than others.

### Interoperability with C#
Although implemented in F#, the system exposes behavior through interfaces and shared contracts, allowing the same logic to be reused or extended from C# without restructuring the architecture.

---

## AI Reliability & Fallback Strategy

LLM providers can fail for reasons outside the application’s control, including quota limits, temporary unavailability, or model lifecycle changes. The system is designed to remain functional and observable under these conditions.

### Provider Abstraction
LLM usage is abstracted behind a common query-extraction interface. This allows multiple providers (Gemini, OpenAI) to be supported without changing the downstream pipeline.

### Retries and Model Fallback
Transient failures (e.g. 429, 503) are retried with exponential backoff. If a model is unavailable or unsupported, the extractor automatically attempts the next configured model.

### Graceful Degradation
If all LLM attempts fail or produce no meaningful signal, the system falls back to a deterministic extractor that derives keywords directly from the input. This ensures the discovery workflow continues to function even when AI services are unavailable.

### Observability
The response includes informational messages indicating which provider and model were used, or whether fallback logic was applied. This makes system behavior transparent during both development and review.

---

# Quick Improvements and Additonal Features:

- Additional tests to test all functions and surface area of small application
- Generic endpoint that can discern what model is being selected based on a dropdown on the client, if multiple configs available.
- More robust UI and interactions for exploring and getting recommendations.
- Track your reading list
- Dropdown to also search other media sources, e.g. movies or art work?


Below is the boilerplate scaffolding README, which is still applicable! The notes above are my additions to the template.

# SAFE Template

This template can be used to generate a full-stack web application using the [SAFE Stack](https://safe-stack.github.io/). It was created using the dotnet [SAFE Template](https://safe-stack.github.io/docs/template-overview/). If you want to learn more about the template why not start with the [quick start](https://safe-stack.github.io/docs/quickstart/) guide?

## Install pre-requisites

You'll need to install the following pre-requisites in order to build SAFE applications

* [.NET SDK](https://www.microsoft.com/net/download) 8.0 or higher
* [Node 18](https://nodejs.org/en/download/) or higher
* [NPM 9](https://www.npmjs.com/package/npm) or higher

## Restoring the project

To get the correct tools and packages installed for the dotnet run command to work, run the following to install the required packages:

```bash
dotnet tool restore
dotnet paket restore
```

## Starting the application

To concurrently run the server and the client components in watch mode use the following command:

```bash
dotnet run
```

Then open `http://localhost:8080` in your browser.

The build project in root directory contains a couple of different build targets. You can specify them after `--` (target name is case-insensitive).

To run concurrently server and client tests in watch mode (you can run this command in parallel to the previous one in new terminal):

```bash
dotnet run -- WatchRunTests
```

Client tests are available under `http://localhost:8081` in your browser and server tests are running in watch mode in console.

Finally, there are `Bundle` and `Azure` targets that you can use to package your app and deploy to Azure, respectively:

```bash
dotnet run -- Bundle
dotnet run -- Azure
```

## SAFE Stack Documentation

If you want to know more about the full Azure Stack and all of it's components (including Azure) visit the official [SAFE documentation](https://safe-stack.github.io/docs/).

You will find more documentation about the used F# components at the following places:

* [Saturn](https://saturnframework.org/)
* [Fable](https://fable.io/docs/)
* [Elmish](https://elmish.github.io/elmish/)
